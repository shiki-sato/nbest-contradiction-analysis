{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Dataset\n",
    "\n",
    "## Final output files are:\n",
    "1. **\"./final_data/ul_train_data_from_nli_contradict.tsv\"**<br>\n",
    "   &emsp;Data set used for unlikelihood training (train data). Synthesized from NLI contradict samples.\n",
    "\n",
    "2. **\"./final_data/ul_train_data_from_nli_entailment.tsv\"**<br>\n",
    "   &emsp;Data set used for unlikelihood training (train data). Synthesized from NLI entailment samples.\n",
    "\n",
    "3. **\"./final_data/ul_valid_data_from_nli_contradict.tsv\"**<br>\n",
    "   &emsp;Data set used for unlikelihood training (valid data). Synthesized from NLI contradict samples.\n",
    "\n",
    "4. **\"./final_data/ul_valid_data_from_nli_entailment.tsv\"**<br>\n",
    "   &emsp;Data set used for unlikelihood training (valid data). Synthesized from NLI entailment samples.\n",
    "\n",
    "5. **\"./final_data/test_data_from_nli_contradict.tsv\"**<br>\n",
    "   &emsp;Data set used for our analyses. Synthesized from NLI contradict samples.\n",
    "   \n",
    "6. **\"./final_data/test_data_from_nli_entailment.tsv\"**<br>\n",
    "   &emsp;Data set used for our analyses. Synthesized from NLI entailment samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download datas\n",
    "We use the Multi-Genre Natural Language Inference (MultiNLI) corpus to synthesize stimulus inputs.<br>\n",
    "See https://aclanthology.org/N18-1101/ for the details of the MultiNLI corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -q https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip -nc\n",
    "unzip -q multinli_1.0.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Split datas by domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def collect_genre(in_fname):\n",
    "    genre_set = set()\n",
    "    with open(in_fname) as in_f:\n",
    "        for l in in_f:\n",
    "            js = json.loads(l.strip())\n",
    "            if not js['genre'] in genre_set:\n",
    "                genre_set.add(js['genre'])\n",
    "    return genre_set\n",
    "\n",
    "\n",
    "def write_genre_file(in_fname, out_fname, genre):\n",
    "    with open(in_fname) as in_f, open(out_fname, 'w') as out_f:\n",
    "        for l in in_f:\n",
    "            js = json.loads(l.strip())\n",
    "            if js['genre'] == genre:\n",
    "                out_f.write('{}\\t{}\\t{}\\n'.format(js['sentence1'], js['sentence2'], js['gold_label']))\n",
    "            \n",
    "\n",
    "def split_by_genre(in_fname, out_dname, suffix_fname='train'):\n",
    "    genre_set = collect_genre(in_fname)\n",
    "    print('Genre Set: {}'.format(genre_set))\n",
    "    for genre in list(genre_set):\n",
    "        out_fname = f'{genre}_{suffix_fname}.tsv'\n",
    "        print('Writing to: {}'.format(out_fname))\n",
    "        write_genre_file(in_fname, os.path.join(out_dname, out_fname), genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre Set: {'telephone', 'slate', 'travel', 'government', 'fiction'}\n",
      "Writing to: telephone_train.tsv\n",
      "Writing to: slate_train.tsv\n",
      "Writing to: travel_train.tsv\n",
      "Writing to: government_train.tsv\n",
      "Writing to: fiction_train.tsv\n"
     ]
    }
   ],
   "source": [
    "in_fname = 'multinli_1.0/multinli_1.0_train.jsonl'\n",
    "out_dname = 'tmp'\n",
    "split_by_genre(in_fname, out_dname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Create questions from \"TELEPHONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform hypothesis into yes-no questions\n",
    "e.g., \"I like it\" -> \"Do you like it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/work/shiki/release_sigdial2022/create_questions/venv-create-questions/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm | grep -v 'already satisfied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 83348/83348 [15:54<00:00, 87.34it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 src/make_question_for_mnli.py \\\n",
    "    --in-fname 'tmp/telephone_train.tsv' \\\n",
    "    --out-fname 'tmp/telephone_train_q.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_of_label(in_fname, out_fname, label):\n",
    "    with open(in_fname) as i_f, open(out_fname,'w') as o_f:\n",
    "        for l in i_f:\n",
    "            c, gr, r, b = l.strip().split('\\t')\n",
    "            if b == label:\n",
    "                o_f.write('{}\\t{}\\t{}\\n'.format(c,gr,r))\n",
    "                \n",
    "in_fname = 'tmp/telephone_train_q.tsv'\n",
    "out_fname = 'tmp/telephone_train_q_e.tsv'\n",
    "get_sample_of_label(in_fname, out_fname, 'entailment')\n",
    "in_fname = 'tmp/telephone_train_q.tsv'\n",
    "out_fname = 'tmp/telephone_train_q_c.tsv'\n",
    "get_sample_of_label(in_fname, out_fname, 'contradiction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative questions\n",
    "e.g., \"Do you like it?\" -> \"Don't you like it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/transform_negative_for_mnli.py \\\n",
    "    --in-fname 'tmp/telephone_train_q_c.tsv' \\\n",
    "    --out-fname 'tmp/telephone_train_q_c_neg.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make it for unlikelihood training\n",
    "!python3 src/transform_negative_for_mnli.py \\\n",
    "    --in-fname 'tmp/telephone_train_q_e.tsv' \\\n",
    "    --out-fname 'tmp/telephone_train_q_e_neg.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make pairs for unlikelihood training\n",
    "Some premise sentences in the MultiNLI corpus have both contradicting and entailing hypotheses.<br>\n",
    "We call these samples paired samples and use them for unlikelihood training.<br>\n",
    "The remaining non-paired samples are used as stimulus inputs for the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_context_lis, e_response_lis, e_question_lis, e_negquestion_lis = [], [], [], []\n",
    "with open('tmp/telephone_train_q_e_neg.tsv') as f:\n",
    "    for l in f:\n",
    "        e_context, e_response, e_question, e_negquestion = l.strip().split('\\t')\n",
    "        assert e_context and e_response and e_question and e_negquestion\n",
    "        e_context_lis.append(e_context)\n",
    "        e_response_lis.append(e_response)\n",
    "        e_question_lis.append(e_question)\n",
    "        e_negquestion_lis.append(e_negquestion)\n",
    "assert len(e_context_lis) == len(set(e_context_lis))\n",
    "\n",
    "c_context_lis, c_response_lis, c_question_lis, c_negquestion_lis = [], [], [], []\n",
    "with open('tmp/telephone_train_q_c_neg.tsv') as f:\n",
    "    for l in f:\n",
    "        c_context, c_response, c_question, c_negquestion = l.strip().split('\\t')\n",
    "        assert c_context and c_response and c_question and c_negquestion\n",
    "        c_context_lis.append(c_context)\n",
    "        c_response_lis.append(c_response)\n",
    "        c_question_lis.append(c_question)\n",
    "        c_negquestion_lis.append(c_negquestion)\n",
    "assert len(c_context_lis) == len(set(c_context_lis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract paired samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/telephone_train_q_e_paired.tsv', 'w') as of_ep, \\\n",
    "        open('tmp/telephone_train_q_c_paired.tsv', 'w') as of_cp:\n",
    "    for e_idx, (e_context, e_response, e_question, e_negquestion) \\\n",
    "            in enumerate(zip(e_context_lis, e_response_lis, e_question_lis, e_negquestion_lis)):\n",
    "        if e_context in c_context_lis:\n",
    "            of_ep.write(f\"{e_context}\\t{e_response}\\t{e_question}\\t{e_negquestion}\\n\")\n",
    "            c_idx = c_context_lis.index(e_context)\n",
    "            of_cp.write(f\"{e_context}\\t{c_response_lis[c_idx]}\\t{c_question_lis[c_idx]}\\t{c_negquestion_lis[c_idx]}\\n\")\n",
    "            # remove saved text for next process\n",
    "            e_context_lis[e_idx], e_response_lis[e_idx], e_question_lis[e_idx], e_negquestion_lis[e_idx] = '', '', '', ''\n",
    "            c_context_lis[c_idx], c_response_lis[c_idx], c_question_lis[c_idx], c_negquestion_lis[c_idx] = '', '', '', ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract unpaired samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp/telephone_train_q_e_unpaired.tsv', 'w') as of_eup, \\\n",
    "        open('tmp/telephone_train_q_c_unpaired.tsv', 'w') as of_cup:\n",
    "    for e_idx, (e_context, e_response, e_question, e_negquestion) \\\n",
    "            in enumerate(zip(e_context_lis, e_response_lis, e_question_lis, e_negquestion_lis)):\n",
    "        if e_context:  # not null after the preceding process = unpaired sample\n",
    "            assert e_response and e_question and e_negquestion\n",
    "            of_eup.write(f\"{e_context}\\t{e_response}\\t{e_question}\\t{e_negquestion}\\n\")\n",
    "    for c_idx, (c_context, c_response, c_question, c_negquestion) \\\n",
    "            in enumerate(zip(c_context_lis, c_response_lis, c_question_lis, c_negquestion_lis)):\n",
    "        if c_context:  # not null after the preceding process = unpaired sample\n",
    "            assert c_response and c_question and c_negquestion\n",
    "            of_cup.write(f\"{c_context}\\t{c_response}\\t{c_question}\\t{c_negquestion}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare final data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare training data for unlikelihood training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir final_data\n",
    "head -n 1800 \"tmp/telephone_train_q_c_paired.tsv\" > \"final_data/ul_train_data_from_nli_contradict.tsv\"\n",
    "head -n 1800 \"tmp/telephone_train_q_e_paired.tsv\" > \"final_data/ul_train_data_from_nli_entailment.tsv\"\n",
    "head -n 2000 \"tmp/telephone_train_q_c_paired.tsv\" | tail -n 200 > \"final_data/ul_valid_data_from_nli_contradict.tsv\"\n",
    "head -n 2000 \"tmp/telephone_train_q_e_paired.tsv\" | tail -n 200 > \"final_data/ul_valid_data_from_nli_entailment.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare test data for analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 2000 \"tmp/telephone_train_q_c_unpaired.tsv\" > \"final_data/test_data_from_nli_contradict.tsv\"\n",
    "head -n 2000 \"tmp/telephone_train_q_e_unpaired.tsv\" > \"final_data/test_data_from_nli_entailment.tsv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv-create-questions': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c139c8d9d70c2834f423bdee6cbadfbadee24cad150b427a689774237c5a2470"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
